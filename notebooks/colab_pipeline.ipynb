{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Pipeline - Google Colab Controller\n",
        "\n",
        "This notebook allows you to run the vision pipeline on Google Colab using a GPU.\n",
        "It mounts Google Drive for storage and sets up the environment automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Clone Repository & Install Dependencies\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "REPO_URL = \"https://github.com/ppopgi-pang/ppopgipang-vision-auto-labeler.git\" # @param {type:\"string\"}\n",
        "BRANCH = \"main\" # @param {type:\"string\"}\n",
        "# ---------------------\n",
        "\n",
        "# Clone repository\n",
        "repo_name = REPO_URL.split(\"/\")[-1].replace(\".git\", \"\")\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"Cloning {REPO_URL}...\")\n",
        "    !git clone {REPO_URL}\n",
        "else:\n",
        "    print(f\"Repository {repo_name} already exists.\")\n",
        "\n",
        "# Move into the project directory\n",
        "%cd {repo_name}\n",
        "\n",
        "# Install dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"requirements.txt not found, installing default dependencies...\")\n",
        "    # Fallback based on pyproject.toml inspection\n",
        "    !pip install pyyaml pydantic pydantic-settings python-dotenv requests pillow numpy opencv-python-headless ImageHash\n",
        "    !pip install torch torchvision transformers ultralytics openai playwright nest_asyncio\n",
        "\n",
        "print(\"Installing system dependencies for Playwright...\")\n",
        "!apt-get update\n",
        "!apt-get install -y libatk1.0-0 libatk-bridge2.0-0 libgtk-3-0 libnss3 libx11-xcb1 \\\\\n",
        "  libxcomposite1 libxdamage1 libxrandr2 libgbm1 libasound2 libpangocairo-1.0-0 \\\\\n",
        "  libpango-1.0-0 libcups2 libdrm2 libxkbcommon0 libxfixes3 libxext6\n",
        "print(\"Installing Playwright browsers...\")\n",
        "!playwright install chromium\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52159a2e",
      "metadata": {},
      "source": [
        "## 1.1 API Keys Configuration\n",
        "**Important:** Run this cell BEFORE importing any project modules to ensure keys are loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35aba41e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# @markdown ### API Keys\n",
        "# @markdown Enter your API keys here. These are required for crawling and LLM verification.\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Inject into environment\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Validation Warning\n",
        "if \"YOUR_\" in OPENAI_API_KEY:\n",
        "    print(\"WARNING: Default placeholder detected for OPENAI_API_KEY. Please update with actual keys.\")\n",
        "    \n",
        "print(\"Environment variables set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. GPU Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"SUCCESS: GPU is available - {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"WARNING: GPU not found. Please enable it in Runtime > Change runtime type > Hardware accelerator > GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Google Drive Mount & Path Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup Paths\n",
        "PROJECT_ROOT = os.getcwd()\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "    print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
        "\n",
        "# IMPORTANT: Add vision_pipeline to sys.path to allow imports from 'pipelines', 'domain', etc.\n",
        "PIPELINE_ROOT = os.path.join(PROJECT_ROOT, 'vision_pipeline')\n",
        "if PIPELINE_ROOT not in sys.path:\n",
        "    sys.path.append(PIPELINE_ROOT)\n",
        "    print(f\"Added {PIPELINE_ROOT} to sys.path\")\n",
        "\n",
        "# Configure Output Directory on Drive\n",
        "BASE_OUTPUT_DIR = \"/content/drive/MyDrive/vision_pipeline_data\" # @param {type:\"string\"}\n",
        "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Set Environment Variable to override config.py default\n",
        "os.environ[\"OUTPUT_DIR\"] = BASE_OUTPUT_DIR\n",
        "print(f\"Output directory set to: {BASE_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pipeline Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from vision_pipeline.domain.job import Job\n",
        "from vision_pipeline.services.pipeline_runner import PipelineRunner\n",
        "from vision_pipeline.config import settings\n",
        "import logging\n",
        "\n",
        "# --- Run Configuration ---\n",
        "KEYWORDS = \"anime character doll, anime figure doll, anime plush doll, anime character plush, anime collectible doll, anime vinyl figure, anime chibi doll, anime character toy, anime action figure, anime mascot doll, \u30a2\u30cb\u30e1 \u30ad\u30e3\u30e9\u30af\u30bf\u30fc \u4eba\u5f62, \u30a2\u30cb\u30e1 \u30d5\u30a3\u30ae\u30e5\u30a2, \u30a2\u30cb\u30e1 \u306c\u3044\u3050\u308b\u307f, \u30ad\u30e3\u30e9\u30af\u30bf\u30fc \u30d5\u30a3\u30ae\u30e5\u30a2, \u30ad\u30e3\u30e9\u30af\u30bf\u30fc \u306c\u3044\u3050\u308b\u307f, \u7f8e\u5c11\u5973 \u30d5\u30a3\u30ae\u30e5\u30a2, \u30c7\u30d5\u30a9\u30eb\u30e1 \u30d5\u30a3\u30ae\u30e5\u30a2, \u30a2\u30cb\u30e1 \u30b0\u30c3\u30ba \u4eba\u5f62, \u30ad\u30e3\u30e9\u30af\u30bf\u30fc \u30c9\u30fc\u30eb, \u30d5\u30a3\u30ae\u30e5\u30a2 \u5199\u771f, \uc560\ub2c8 \uce90\ub9ad\ud130 \uc778\ud615, \uc560\ub2c8 \ud53c\uaddc\uc5b4, \uce90\ub9ad\ud130 \uc778\ud615, \uce90\ub9ad\ud130 \ud53c\uaddc\uc5b4, \uc560\ub2c8\uba54\uc774\uc158 \uc778\ud615, \uc560\ub2c8 \uad7f\uc988 \uc778\ud615, \uce90\ub9ad\ud130 \uad7f\uc988 \ud53c\uaddc\uc5b4, \ubbf8\uc18c\ub140 \ud53c\uaddc\uc5b4, SD \ud53c\uaddc\uc5b4, \uc560\ub2c8 \uce90\ub9ad\ud130 \uc7a5\ub09c\uac10, anime doll figure, anime plush toy, anime figure collection, anime doll collection, anime figure photography, anime toy figure, anime doll toy, anime character merchandise doll, anime figure close up, anime doll product photo, anime figure real photo, anime doll real life, anime figure unboxing, anime figure review, anime plush doll photo, anime figure shelf, anime figure display, anime doll collection photo, anime figure product shot, anime figure desk setup, anime plush, anime stuffed doll, character plush doll, anime soft toy, anime mascot plush, anime plush collection, character plush toy, anime doll plush, anime plush figure, anime plush photo, anime scale figure, anime PVC figure, anime resin figure, anime Nendoroid, anime garage kit, anime action figure toy, anime figure model, anime collectible figure, anime statue figure, anime figure closeup, Japanese character doll, Japanese anime figure, otaku figure collection, otaku room figure, anime goods figure, anime hobby figure, anime character merchandise, anime toy collection, anime doll merchandise, anime figure goods, anime doll photo site:jp, anime figure photo site:jp, \u30a2\u30cb\u30e1 \u30d5\u30a3\u30ae\u30e5\u30a2 \u5199\u771f, \u30ad\u30e3\u30e9\u30af\u30bf\u30fc \u4eba\u5f62 \u5199\u771f, anime plush photo site:jp, \u30d5\u30a3\u30ae\u30e5\u30a2 \u5b9f\u7269 \u5199\u771f, anime figure blog, anime doll review blog, \u30d5\u30a3\u30ae\u30e5\u30a2 \u30ec\u30d3\u30e5\u30fc, \u30a2\u30cb\u30e1 \u30b0\u30c3\u30ba \u5199\u771f\" # @param {type:\"string\"}\n",
        "TARGET_OBJECT = \"doll\" # @param {type:\"string\"}\n",
        "LIMIT = 50000 # @param {type:\"integer\"}\n",
        "\n",
        "# Parse keywords (split by space if needed, or treat as single phrase if preferred)\n",
        "# The CLI treats multiple args as list. Here we take one string and split if multiple terms intended, \n",
        "# or just wrap in list.\n",
        "keyword_list = [k.strip() for k in KEYWORDS.split(',') if k.strip()]\n",
        "if not keyword_list:\n",
        "    keyword_list = [KEYWORDS]\n",
        "\n",
        "print(f\"Processing Keywords: {keyword_list}\")\n",
        "print(f\"Target Object: {TARGET_OBJECT}\")\n",
        "print(f\"Limit: {LIMIT}\")\n",
        "print(f\"Saving to: {settings.output_dir}\")\n",
        "\n",
        "def run_pipeline():\n",
        "    # Construct Job ID\n",
        "    first_kw = keyword_list[0].replace(' ', '_')\n",
        "    job_id = f\"job_{TARGET_OBJECT}_{first_kw}\"\n",
        "    \n",
        "    # Check for existing job data to avoid accidental overwrite/re-run if not desired\n",
        "    # (The pipeline logic itself handles skipping ideally, but we warn)\n",
        "    job_path = os.path.join(settings.output_dir, job_id)\n",
        "    if os.path.exists(job_path):\n",
        "        print(f\"\\n[INFO] Job directory {job_path} already exists. Pipeline will attempt to resume or skip accomplished steps.\")\n",
        "    \n",
        "    # Create Job\n",
        "    job = Job(\n",
        "        keywords=keyword_list,\n",
        "        target_class=TARGET_OBJECT,\n",
        "        limit=LIMIT,\n",
        "        job_id=job_id\n",
        "    )\n",
        "    \n",
        "    # Run\n",
        "    runner = PipelineRunner()\n",
        "    try:\n",
        "        runner.run(job)\n",
        "        print(\"\\nWith Great Success! Pipeline finished.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Pipeline failed: {e}\")\n",
        "        logging.exception(\"Pipeline Failure\")\n",
        "\n",
        "# Execute\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}